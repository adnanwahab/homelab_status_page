5. EThics, Safetey, robustness


Latent scope

Neura link
https://www.youtube.com/watch?v=Kbk9BiPhm7o&ab_channel=LexFridman






# Simulation

using this simulation - drive robot around -> like mario kart

Robot mimics your actions - multiplayer in browser like quake live

<iframe src="https://gpu.jerboa-kokanue.ts.net/" style="width:100%; height: 900px">


https://www.youtube.com/watch?v=qC5KtatMcUw
Browser base64 Session Description<br />
<textarea id="localSessionDescription" readonly="true"></textarea> <br />
<button onclick="window.copySDP()">
	Copy browser SDP to clipboard
</button>
<br />
<br />

Golang base64 Session Description<br />
<textarea id="remoteSessionDescription"></textarea> <br/>
<button onclick="window.startSession()"> Start Session </button><br />

<br />

Video<br />
<video id="video1" width="160" height="120" autoplay muted></video> <br />

Logs<br />
<div id="logs"></div>

<script>

const pc = new RTCPeerConnection({
  iceServers: [
    {
      urls: 'stun:stun.l.google.com:19302'
    }
  ]
})
const log = msg => {
  document.getElementById('logs').innerHTML += msg + '<br>'
}

navigator.mediaDevices.getUserMedia({ video: true, audio: true })
  .then(stream => {
    stream.getTracks().forEach(track => pc.addTrack(track, stream))
    document.getElementById('video1').srcObject = stream
    pc.createOffer().then(d => pc.setLocalDescription(d)).catch(log)
  }).catch(log)

pc.oniceconnectionstatechange = e => log(pc.iceConnectionState)
pc.onicecandidate = event => {
  if (event.candidate === null) {
    document.getElementById('localSessionDescription').value = btoa(JSON.stringify(pc.localDescription))
  }
}

window.startSession = () => {
  const sd = document.getElementById('remoteSessionDescription').value
  if (sd === '') {
    return alert('Session Description must not be empty')
  }

  try {
    pc.setRemoteDescription(JSON.parse(atob(sd)))
  } catch (e) {
    alert(e)
  }
}

window.copySDP = () => {
  const browserSDP = document.getElementById('localSessionDescription')

  browserSDP.focus()
  browserSDP.select()

  try {
    const successful = document.execCommand('copy')
    const msg = successful ? 'successful' : 'unsuccessful'
    log('Copying SDP was ' + msg)
  } catch (err) {
    log('Unable to copy SDP ' + err)
  }
}
</script>

# unreal


# isaac-ROS
https://docs.omniverse.nvidia.com/isaacsim/latest/installation/manual_livestream_clients.html#omniverse-streaming-client
unreal pixel streaming


# Ethics, Safety and Neural-Interfaces

<REWROD ME> -- llm must reword - users can contribute a datum
if humans become slightly smarter than robots will be much smarter
to keep humans competitive with robots -> lets make a human cognition engine
https://github.com/uwdata/living-papers
https://idl.uw.edu/papers/living-papers

Everything that exists is a starting point, or a mid-point.
Nothing is complete.
</REWORDME>


# Open-sauce = yay
//if you do pass a code - then we  write a tempfile to one of the folders... note sure which one --- each notebook gens a folder ---- which probably improts the otherS? or those are all functions in one folder - which your code can call --
	//bun_examples = one notebok
	//deno_wxamples = one notebook
	//---webgpu examples - child notebook
	//zig examples = one notebook (make sure this is actually useful - but probably easier than other "system" langueages - index HN )
	///get all 4 done by 12
	///that way - server doesnt have to do all the ops and random tools - it can just use std lib - -- to leverage other tools - minimal as possible ---- just a blag --- just back to a status page - for all the other modules to build dynamic lands .

	// green means === 100% public - 98% of data + code
	// greenish-yellow -*1e-8 or  10 of 1000() auditors - whose corporate-actions are green

	/// yellow - 1% - 70 million auditors
	/// source - nyc opendata office - mayor.bloomberg = cool
	///goal - further RMS - research - bc - all logs + most non-essential code = good for people to know - in case of bug - but IP and red-level secrets should be 100% private --- unless theres an audit by ~ 100% of the worlds population.








----------





# Neuralink

# Law
---
toc: false
---

# 50,000 U.S. Federal Laws

This [fascinating dataset](https://osf.io/mrghc/?view_only) contains the titles and dates of almost 50,000 U.S. federal laws from ${min(da, d => d.date_of_passage).split("-")[0]} to ${max(da, d => d.date_of_passage).split("-")[0]}.
I was made aware of this dataset in the [Data is Plural 2024.02.28 edition](https://www.data-is-plural.com/archive/2024-02-28-edition/), a great place to find interesting public datasets.

The 49,746 laws:

<div class="input-card">
<div class="static-table">
${Inputs.table(da, tableConfig)}
</div>
</div>

As you might imagine, 50k laws is a lot to sort through.
We can use [Latent Scope](https://github.com/enjalot/latent-scope) to organize them by the similarity of their titles and see what clusters of laws emerge.
This article takes you through a short tour of these clusters, stopping at some really funny laws as well as some deadly serious laws.

Before we explore, I want to give a short preview of what Latent Scope is, you can get a full tutoria on [building your own scope](your-first-scope) but this is the interface you would work with to curate your clusters in the tool:

<img src="/assets/us-federal-laws/explore.png" class="pageshot">

Latent scope helps you put your data through a [4 step process](your-first-scope):

<img src="/assets/us-federal-laws/setup.png" class="screenshot">

1. Embed - run each piece of text through an embedding model
2. Project - run the high-dimensional embeddings through UMAP
3. Cluster - run the 2-dimensional UMAP coordinates through HDBSCAN
4. Label - ask an LLM to create a label by summarizing a list of text taken from each cluster

So at the end of this process we have ${clusterTableData.length} clusters carving up our ${da.length} laws, with
every row of our input data annotated with a cluster index and label.
In our other example analyses like [Datavis Survey](datavis-survey) and [enjalot's tweets](enjalot-tweets) we didn't have quite so many data points.
50,000 isn't enormous, Latent Scope can easily work with a million on an average computer, but it is still a lot of points to plot.

So in this analysis we will focus on exploring the ${clusterTableData.length} clusters, showing the individual laws in each cluster only when you select a cluster.
Feel free to peruse the clusters in this interface, if you keep scrolling I'll take you on a tour of the ones I find most interesting!

_Click on the radio button on the left of each cluster in the table, or click on a cluster in the map to select it and see the details in the card below_

<div class="input-card" style="border: 1px solid lightgray; border-radius: 5px; padding: 10px;">

```js
const clusterTableData = scope.cluster_labels_lookup.map(c => {
  let dc = da.filter(d => d.cluster == c.cluster)
  return {
    cluster: c.cluster,
    label: c.label,
    count: dc.length,
    min_date: min(dc, d => d.date_of_passage),
    max_date: max(dc, d => d.date_of_passage),
  }
})
const selclusterTable = view(Inputs.table(clusterTableData, {
  width: {
    cluster: 40,
  },
  header: {
    "cluster": ""
  },
  sort: "min_date",
  reverse: false,
  multiple: false,
  value: clusterTableData[271]
}))
```

<div style="position:relative;">

  ```js
  const dynahull = view(dynamicHull(hulls, {
    width: 500,
    height: 500,
    xd: [-1.1, 1.1],
    yd: [-1.1, 1.1],
    selected: selclusterTable.cluster
  }))
  ```


  <div>
    ${tip}
  </div>
</div>
</div>

```js
function clusterDescription (cluster) {
  return htl.html`<div>
   <span style="border-bottom: 2px solid lightblue">favorites: ${clusterTableData[cluster].favorites}</span>
   <br/>
   <br/>
   <span style="border-bottom: 2px solid orange">retweets: ${clusterTableData[cluster].retweets}</span>
  </div>`
}
```

<div>
  ${clusterCard(dynahull.selected, {
    description: "",
    plot: dateBars(dynahull.selected),
    tableConfig,
    da,
    scope,
    hulls,
    r: 1
  })}
</div>
<br/>

## ${clusterTableData[10].label}
The cluster that jumped out at me when I was first browsing the data is **${clusterTableData[10].label}**.
Seeing laws with a title like
*"To change the name of the schooner Sally McGee to that of Ocean Eagle"* and
*"To authorize the Secretary of the Treasury to change the name of the steam yacht "Fanny."* made me wonder what was going on in the 1800s.
It seems like back then you even had to get Congress to help you fix a typo in your boat name: *"	To change the name of the schooner La Pette to La Petite."*

<div>
  ${clusterCard(10, {
    plot: "You used to have to ask Uncle Sam to name your boat?",
    description: dateBars(10),
    tableConfig,
    da,
    scope,
    hulls,
    r: 1
  })}
</div>
<br/>

## ${clusterTableData[59].label}
Another fun cluster is **${clusterTableData[59].label}**.
Were you aware that since 1962 the U.S. has had a "an annual National School Lunch Week"?
There are over 600 National awareness related laws in this cluster!

<div>
  ${clusterCard(59, {
    plot: "This is a lot to be aware of...",
    description: dateBars(59),
    tableConfig,
    da,
    scope,
    hulls,
    r: 1
  })}
</div>

I also found the *"Joint resolution to designate the third Sunday in June 1966 as Father's Day"*, which made me curious about when Mother's day was establshed.
I believe this dataset is missing the [1914 law](https://en.wikipedia.org/wiki/Mother%27s_Day_(United_States)#Establishment_of_holiday) passed by U.S. Congress to make it official.

Here are the ${da.filter(d => d.date_of_passage?.indexOf("1914-05") == 0).length} laws in the dataset passed in May of 1914:
<div class="input-card">
${Inputs.table(da.filter(d => d.date_of_passage?.indexOf("1914-05") == 0), tableConfig)}
</div>

When dealing with a lot of data, small data quality issues can be hard to find.
Being able to explore the data by through the meaningful groupings of the clusters is a great way to ask questions you didn't know you had!


## U.S. Land Legislation
An interesting and a bit more serious cluster is **${clusterTableData[189].label}**.
Can you find laws related the Louisiana Purchase around 1803?

<div>
  ${clusterCard(189, {
    description: "",
    plot: dateBars(189),
    tableConfig,
    da,
    scope,
    hulls,
    r: 1
  })}
</div>

## Bridge Building
There are more than 3,000 laws related to bridge building and dam construction across 19 clusters.
Sometimes the embedding model finds patterns in text that are more subtle than a high level concept such as bridges and dams.
Investigating what the different patterns in each cluster are could lead to interesting insights (if you care about bridges and dams!)

```js
const bridgeClusters = [72, 73, 74, 75, 76, 77, 79, 80,81, 83, 85, 88, 89, 90, 91, 92, 93, 97, 98]
```
<div>
${clusterListCard(bridgeClusters, {
    heading: "Bridge Building",
    plot: dateBars(bridgeClusters),
    tableConfig: {...tableConfig, columns: ["Title", "date_of_passage", "cluster", "label"],
      width: { Title: "70%", date_of_passage: 100, cluster: 40, label: 170}, sort: "cluster"},
    da,
    scope,
    hulls
})}
</div>

Another thing to notice about those bridge building clusters is that a lot of those laws were passed in the early 20th century, with a peak in the late 1920s and early 1930s.
This is also around the time that the Golden Gate Bridge began construction, so we must have really been into bridges at that time.

## Washington D.C.
What does Congress love almost as much as building bridges? Washington D.C.!
With over 5% of the laws in the dataset found in the Washington D.C. cluster, you can tell they really care about it.

```js
const dcClusters = [123, 106, 107, 109, 110, 111, 112, 114, 116, 118, 119]
```
<div>
${clusterListCard(dcClusters, {
    heading: "Washington D.C.",
    plot: dateBars(dcClusters),
    tableConfig: {...tableConfig, columns: ["Title", "date_of_passage", "cluster", "label"], width: { Title: "70%", date_of_passage: 100, cluster: 40, label: 170}},
    da,
    scope,
    hulls
})}
</div>
<br/>

## Native Americans
There are also quite a few laws relating to our sad history with Native Americans.

```js
const naClusters = [105, 127, 141, 146,  178, 183, 190, 204, 205, 210, 211]
```
<div>
${clusterListCard(naClusters, {
    heading: "Native American",
    plot: dateBars(naClusters),
    tableConfig: {...tableConfig, columns: ["Title", "date_of_passage", "cluster", "label"], width: { Title: "70%", date_of_passage: 100, cluster: 40, label: 170}},
    da,
    scope,
    hulls
})}
</div>
<br/>

## Civil Rights

I wasn't able to identify clusters that related specifically to civil rights, but thanks to the embeddings I was able to use nearest neighbor search to find 150 laws that are related to the concept of civil rights.
```js
const civilRights = (await FileAttachment("data/us-federal-laws/Civil Rights.indices").text()).split("\n").map(d => da.find(p => p.index == +d))
```

<div class="card">

```js
 Plot.plot({
        marks: [
          Plot.hull(hulls.flatMap(d => d), {
            x: "x",
            y: "y",
            fill: "cluster",
            fillOpacity: 0.1,
            stroke: "lightgray",
            curve: "catmull-rom",
          }),

          Plot.dot(civilRights, {
            x: "x",
            y: "y",
            fill: "cluster",
            title: d => `${d.Title}\n${d.date_of_passage}\n${d.cluster}: ${d.label}`,
            tip: true
          }),
        ],
        width: 500,
        height: 500,
        color: { scheme: "cool" },
        y: { axis: null},
        x: { axis: null },
        tip: {
          format: {
            cluster: true,
            title: true,
            date_of_passage: true
          }
        }
      })
```

  <div class="static-table">
    ${Inputs.table(civilRights,
      {...tableConfig, columns: ["Title", "date_of_passage", "cluster", "label"], width: { Title: "70%", date_of_passage: 100, cluster: 40, label: 170}}
    )}
  </div>
</div>

Nearest neighbor search on embeddings is what powers techniques like Retrieval Augmented Generation (RAG), finding relevant documents to feed to an LLM as context for a query.
In the above visualization we can see that the similarity search on the query of "civil rights" returns laws that are mostly relevant but also spread across various clusters.
What would it be like to leverage the clustering to determine relevance when implementing techniques like RAG?

Interested in running your own large text dataset through Latent Scope? Try out [the getting started tutorial](your-first-scope) and reach out on [Discord](https://discord.gg/x7NvpnM4pY) if you run into any issues!



```js
// const map = view(canvas)
```

```js
const tip = tooltip({})
```

```js
// let hp = da[map.hovered[0]]
 ```

```js
if(dynahull.hovered >= 0){
  // display the tooltip
  // console.log("p, sel", p, selected)
  // calculate the hull centroid
  let h = dynahull.hovered
  let x = mean(hulls[h], d => d.x)
  let y = mean(hulls[h], d => d.y)
  let hp = {...hulls[h][0], x, y}
  tip.show(hp, {width: 500, height: 500, xd: [-1.1, 1.1], yd: [-1.1, 1.1]}, md.unsafe(`
  <i>Cluster ${hp["cluster"]}: ${hp["label"]}</i>
  <br/>
  <span>${min(hulls[h], d => d["date_of_passage"])} - ${max(hulls[h], d => d["date_of_passage"])}</span>
  <br/>
  ${da.filter(d => d.cluster == h).length} laws
  `))
} else {
  tip.hide()
}
```

```js
const tableConfig = {
  columns: [
    "Title",
    "date_of_passage",
  ],
  width: {
    "Title": "80%"
  },
  sort: "date_of_passage",
  reverse: false,
  rows: 15
}
```

```js
// const canvas = document.createElement("canvas")
```

```js
// let tableData = []
// if(map.selected.length > 0) {
//   // tableData = []
//   map.selected.forEach(i => tableData.push(da[i]))
// }
```

```js
// calculate the hull points
const hulls = scope.cluster_labels_lookup.map(c => {
  return c.hull.map(idx => da.find(d => d.index === idx))
})
```

```js
function dateBars(cluster) {
  let filter = d => d.cluster == cluster
  if (Array.isArray(cluster)) {
    filter = d => cluster.indexOf(d.cluster) >= 0
  }
  return Plot.plot({
    marks: [
      Plot.barX(da, Plot.binY({x: "count"}, {
        filter,
        y: d => new Date(d["date_of_passage"]).getFullYear(),
        x: "count",
      }))
    ],
    marginLeft: 80,
    marginBottom: 30,
    width: 300,
    height: 300,
    y: { label: null, tickFormat: x => Math.floor(x).toString() },
    x: { label: null },
    // style: { "background-color": "white" }
  })
}
```


```js
const db = DuckDBClient.of({
  scope: FileAttachment("data/us-federal-laws/scopes-001-input.parquet")
});
```

```js
const scope = FileAttachment("data/us-federal-laws/scopes-001.json").json()
```

```js
// const rows = db.sql`SELECT * FROM input`
const data = db.sql`SELECT index,cluster,label,Title,date_of_passage,source,x,y FROM scope`
```
```js
const da = data.toArray().map(d => d.toJSON())
```
```js
console.log("data!", da)
console.log("scope!", scope)
console.log("hulls!", hulls)
```

```js
import {scatter} from "./components/scatter.js";
import {hull} from "./components/hull.js";
import {dynamicHull} from "./components/dynamicHull.js";
import {tooltip} from "./components/tooltip.js";
import {clusterCard} from "./components/clusterCard.js";
import {clusterListCard} from "./components/clusterListCard.js";

import markdownit from "markdown-it";
import { select } from 'npm:d3-selection';
import { min, max, sum, mean } from 'npm:d3-array';
```
```js
const Markdown = new markdownit({html: true});

const md = {
  unsafe(string) {
    const template = document.createElement("template");
    template.innerHTML = Markdown.render(string);
    return template.content.cloneNode(true);
  }
};
```

```js
const isMobileDevice = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
```
```js
if(isMobileDevice) {
  select(".regl-container").style("display", "none")
}
```



https://opendata.cityofnewyork.us/ - all data public but under 3 layers of filters
green = annoynmized
yellow = audit
red = white-glove audit





# Planning and Prediction

<input type="text" placeholder="ask chatbot anything" />


theres currently a debate at the frontier of robotcs on whether LLMs are good for planning / prediction

for simplicity, we're just going to use LLMs for planning for the next 2-3 years. after that we'll re-evaluate.



he AI stack at the center of the Hashirama driving system broadly consists of three processes, which occur in order: perception, prediction, and planning. These equate to seeing the world and how everything around the vehicle is currently moving, predicting how everything will move next, and deciding how to move from A to B given those predictions.


We humans often lament that we cannot predict the future, but perhaps we don’t give ourselves quite enough credit. With sufficient practice, our short-term predictive skills become truly remarkable.


“Predicting the future — the intentions and movements of other agents in the scene — is a core component of safe, autonomous driving,” says Kai Wang, director of the Zoox Prediction team.


“We don’t need to label any data by hand, because our data show where things actually moved into the future,” says Wang. “My team doesn’t have a data problem. Our main challenge is that the future is inherently uncertain. Even humans cannot do this task perfectly.”


“Think of the GNN as a message-passing system by which all the agents and static elements in the scene are interconnected,” says Mahsa Ghafarianzadeh, senior software engineer on the Prediction team.


“Prediction doesn’t happen in a vacuum. Other people’s behaviors are dependent on how their world is changing. If you’re not capturing that within prediction, you’re limiting yourself,” says Wang.


i asked kai wang what the most change in ai was. kai wang told me in feb 2021 "creative ai has a lot of potential"
Today we call it "generative ai" and its very buzzy. Funny how the director prediction was so good at predicting the future.
I like the name creative-ai better.
And i think by combining robotics with creative-ai, we can make the world easier to edit.
As of 2024, doing great things takes a lot of work. It should, they say.
I dont think so.
I think we can make creating great wonders as easy as wiping your buttocks.

![](https://assets.amazon.science/dims4/default/75dea15/2147483647/strip/true/crop/960x286+0+0/resize/1200x358!/format/webp/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fa1%2F2d%2F65e5fdbc47099eea0a5b6984c55f%2Fanother-one.gif)


1. https://www.amazon.science/latest-news/how-the-zoox-robotaxi-predicts-everything-everywhere-all-at-once




# Perception



<iframe src="http://hashirama.blog/dummy-stream">

For robotics-odyssey - we begin with a simple iphone camera or a webcam.

You can use any camera - an iphone from 2015 that is un-used in your drawer.

And with the magic of gpus, we upscale and imbue that simple camera stream with depth perception and labels that make it competitive with farm more expensive sensors.


We also integrate with state-of-the-art sensors like the ZED camera from stereo-labs, and lidar cameras.

Those higher-resolution sensors do offer a lot, especially for industrial uses caes and safety critical applicaitons.

But for learning the basics, any simple camera will do, even one $20 off amazon - link wirecutter.

Perception quickly identifies and classifies the other cars, pedestrians, and cyclists in the scene, which are dubbed “agents.” And crucially, it tracks each agent’s velocity and current trajectory. These data are then combined with the ZRN to provide the Zoox vehicle with an incredibly detailed understanding of its environment.



These predictions are typically up to about 8 seconds into the future, but they are constantly recalculated every tenth of a second as new information is delivered from Perception.

thats 100ms.

Because they are 8 seconds long, we can caluclate them on the server where we have much more data and compute. Then we can update them periodically

But perception wants to be done on the edge - because want a picture of reality to update every 1ms.


*
*
*
* vision transformer
* semantic segmantion
## Installation

SAM 2 needs to be installed first before use. The code requires `python>=3.10`, as well as `torch>=2.3.1` and `torchvision>=0.18.1`. Please follow the instructions [here](https://pytorch.org/get-started/locally/) to install both PyTorch and TorchVision dependencies. You can install SAM 2 on a GPU machine using:

```bash
git clone https://github.com/facebookresearch/segment-anything-2.git

cd segment-anything-2 & pip install -e .
```
If you are installing on Windows, it's strongly recommended to use [Windows Subsystem for Linux (WSL)](https://learn.microsoft.com/en-us/windows/wsl/install) with Ubuntu.

To use the SAM 2 predictor and run the example notebooks, `jupyter` and `matplotlib` are required and can be installed by:

```bash
pip install -e ".[demo]"
```

Note:
1. It's recommended to create a new Python environment via [Anaconda](https://www.anaconda.com/) for this installation and install PyTorch 2.3.1 (or higher) via `pip` following https://pytorch.org/. If you have a PyTorch version lower than 2.3.1 in your current environment, the installation command above will try to upgrade it to the latest PyTorch version using `pip`.
2. The step above requires compiling a custom CUDA kernel with the `nvcc` compiler. If it isn't already available on your machine, please install the [CUDA toolkits](https://developer.nvidia.com/cuda-toolkit-archive) with a version that matches your PyTorch CUDA version.
3. If you see a message like `Failed to build the SAM 2 CUDA extension` during installation, you can ignore it and still use SAM 2 (some post-processing functionality may be limited, but it doesn't affect the results in most cases).

Please see [`INSTALL.md`](./INSTALL.md) for FAQs on potential issues and solutions.

## Getting Started

### Download Checkpoints
### Image prediction
```python
import torch
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor

checkpoint = "./checkpoints/sam2_hiera_large.pt"
model_cfg = "sam2_hiera_l.yaml"
predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))

with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    predictor.set_image(<your_image>)
    masks, _, _ = predictor.predict(<input_prompts>)
```
```python
import torch
from sam2.build_sam import build_sam2_video_predictor

checkpoint = "./checkpoints/sam2_hiera_large.pt"
model_cfg = "sam2_hiera_l.yaml"
predictor = build_sam2_video_predictor(model_cfg, checkpoint)

with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    state = predictor.init_state(<your_video>)

    # add new prompts and instantly get the output on the same frame
    frame_idx, object_ids, masks = predictor.add_new_points_or_box(state, <your_prompts>):

    # propagate the prompts to get masklets throughout the video
    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):
        ...
```
## Load from 🤗 Hugging Face
```python
import torch
from sam2.sam2_image_predictor import SAM2ImagePredictor

predictor = SAM2ImagePredictor.from_pretrained("facebook/sam2-hiera-large")

with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    predictor.set_image(<your_image>)
    masks, _, _ = predictor.predict(<input_prompts>)
```
For video prediction:
```python
import torch
from sam2.sam2_video_predictor import SAM2VideoPredictor

predictor = SAM2VideoPredictor.from_pretrained("facebook/sam2-hiera-large")

with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    state = predictor.init_state(<your_video>)

    # add new prompts and instantly get the output on the same frame
    frame_idx, object_ids, masks = predictor.add_new_points_or_box(state, <your_prompts>):

    # propagate the prompts to get masklets throughout the video
    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):
        ...
```


---
title: Living Research Papers
---


1. make a rag of

1. alan kay
2. bret victor
3. all robotics research papers
4. laws


use new graphics engine for making gifs / lotties
